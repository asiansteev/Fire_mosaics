---
title: "Growth-stage optimization"
author: "Holly Sitters, Julian Di Stefano, Timothy Wills, Matthew Swan, Alan York"
date: "7 October, 2016"
output: html_document
---

***

## 1 Resample data for each growth stage 1000 times, and combine output in a single data frame

1000 resamples is arbitrary -- a greater number will reduce the confidence intervals around the result, but the difference is very small.

This document assumes four growth stages, so edit if using a different number. 

Save separate input files for each growth stage in the following format (species as rows, sites as columns):

species   | A   | B   | C   | D   | E   | F
----- | --- | --- | --- | --- | --- | ---
sp1   |     |     |     |     |     |
sp2   |     |     |     |     |     |

If data contain subsamples (e.g. quadrats within sites), use the following format (sites are A, B, C; two As correspond to two quadrats/sub-samples):

species   | A   | A   | B   | B   | C   | C
----- | --- | --- | --- | --- | --- | ---
sp1   |     |     |     |     |     |
sp2   |     |     |     |     |     |

Save each spreadsheet as a txt (or csv) file, and read them in. 

```{r Read in data, eval=FALSE}
## Packages needed throughout
library (plyr)
library (dplyr)
library (reshape2)

#setwd("C:/Users/Documents/Example")

# w = data are in wide format; a, b, c, d keep growth stages in order

#select just the species column
plant_spp <- read.csv("data/plant_data_natives.csv", header=TRUE) %>% 
  select(species) 

#now make a list of all species
plant_dat <- read.csv("data/plant_data_natives.csv", header=TRUE)

plant_dat_UN <- plant_dat %>% 
  filter(sev == "u") %>% 
  select(species)
complete_spp <- union(plant_spp, plant_dat_UN)

## Unburnt data: filter to get only UN; keep only species and cover columns
plant_dat_UN <- plant_dat %>% 
  filter(sev == "u") %>% 
  select(species, cover, site_id)  %>% 
  arrange(species)

#plant_dat_UN <- full_join(plant_dat_UNa, plant_dat_UNb, by = "species")%>% 
#  arrange(species)

# make it wide format
plant_dat_UN <- reshape(plant_dat_UN, idvar = "species", timevar = "site_id", direction = "wide")

# add species not detected to get same length vectors
#plant_dat_UN1 

ar.w <- full_join(complete_spp, plant_dat_UN, by = "species") %>% 
  replace(., is.na(.), "0")

## LS data (less than 11 yrs;): filter to get only LS, TSF less than 11 years, keep only species and cover columns
plant_dat_LS1 <- plant_dat %>% 
  filter(sev == "l")

plant_dat_LS1$fire_yr <- as.character(plant_dat_LS1$fire_yr)
plant_dat_LS1$fire_yr <- as.numeric(plant_dat_LS1$fire_yr)
plant_dat_LS1$tsf <- 2018-plant_dat_LS1$fire_yr

plant_dat_LS1 <- plant_dat_LS1 %>% 
  filter(tsf < 11) %>% 
  select(species, cover, site_id) %>% 
  arrange(species)
  
plant_dat_LS1 <- reshape(plant_dat_LS1, idvar = "species", timevar = "site_id", direction = "wide")

# add species not detected to get same length vectors
#plant_dat_LS1 

be.w <- full_join(complete_spp, plant_dat_LS1, by = "species") %>% 
  replace(., is.na(.), "0")

## LS data (less than 10 yrs; 11 sites total): filter to get only LS, TSF more than 10 years, keep only species and cover columns
plant_dat_LS2 <- plant_dat %>% 
  filter(sev == "l")

plant_dat_LS2$fire_yr <- as.character(plant_dat_LS2$fire_yr)
plant_dat_LS2$fire_yr <- as.numeric(plant_dat_LS2$fire_yr)
plant_dat_LS2$tsf <- 2018-plant_dat_LS2$fire_yr

plant_dat_LS2 <- plant_dat_LS2 %>% 
  filter(tsf < 11) %>% 
  select(species, cover, site_id) %>% 
  arrange(species)
  
plant_dat_LS2 <- reshape(plant_dat_LS2, idvar = "species", timevar = "site_id", direction = "wide")

# add species not detected to get same length vectors
#plant_dat_LS1 

xx.w <- full_join(complete_spp, plant_dat_LS2, by = "species") %>% 
  replace(., is.na(.), "0")

# HS data (less than 11 yrs; 13 sites): filter to get only LS, TSF less than 10 years, keep only species and cover columns
plant_dat_HS1 <- plant_dat %>% 
  filter(sev == "h")

plant_dat_HS1$fire_yr <- as.character(plant_dat_HS1$fire_yr)
plant_dat_HS1$fire_yr <- as.numeric(plant_dat_HS1$fire_yr)
plant_dat_HS1$tsf <- 2018-plant_dat_HS1$fire_yr

plant_dat_HS1 <- plant_dat_HS1 %>% 
  filter(tsf < 11) %>% 
  select(species, cover, site_id) %>% 
  arrange(species)

plant_dat_HS1 <- reshape(plant_dat_HS1, idvar = "species", timevar = "site_id", direction = "wide")

#plant_dat_LS11 

cm.w <- full_join(complete_spp, plant_dat_HS1, by = "species") %>% 
  replace(., is.na(.), "0")

# HS data (more than 10 yrs; 10 sites): filter to get only LS, TSF more than 10 years, keep only species and cover columns
plant_dat_HS2 <- plant_dat %>% 
  filter(sev == "h")

plant_dat_HS2$fire_yr <- as.character(plant_dat_HS2$fire_yr)
plant_dat_HS2$fire_yr <- as.numeric(plant_dat_HS2$fire_yr)
plant_dat_HS2$tsf <- 2018-plant_dat_HS2$fire_yr

plant_dat_HS2 <- plant_dat_HS2 %>% 
  filter(tsf > 10) %>% 
  select(species, cover, site_id) %>% 
  arrange(species)

plant_dat_HS2 <- reshape(plant_dat_HS2, idvar = "species", timevar = "site_id", direction = "wide")

#plant_dat_HS22 

dl.w <- full_join(complete_spp, plant_dat_HS2, by = "species") %>% 
  replace(., is.na(.), "0")
```

Only run the following chunk if you need to convert abundance data to presence-absence.

```{r Convert abundance data to presence-absence, eval = FALSE}
convert <- function (x) {
  species <- x %>% select (species)
  s <- x %>% select(-species)
  df <-  data.frame(t(apply (as.matrix(s), 1, function (x) ifelse (x>0, 1, 0))))
  b <- bind_cols (species, df)
  return (b)}

ar.w <- convert (ar.w)
be.w <- convert (be.w)
cm.w <- convert (cm.w)
dl.w <- convert (dl.w)
```

Convert the data format from wide to long (such that there are only three columns: species, site, value), and order by species. Then delete the site column.  

```{r Melt data, warning=FALSE, results="hide", eval = FALSE}
melt.sort <- function (x){
  m <- melt(x, id.vars="species") # convert to long
  s <- m[order(m$species),] # order by species
  s$variable <- NULL # delete site column
  return(s)}

# l = data are in long format
ar.l <- melt.sort(ar.w) %>% 
  mutate(value = as.numeric(value))
be.l <- melt.sort(be.w) %>% 
  mutate(value = as.numeric(value))
xx.l <- melt.sort(xx.w) %>% 
  mutate(value = as.numeric(value))
cm.l <- melt.sort(cm.w) %>% 
  mutate(value = as.numeric(value))
dl.l <- melt.sort(dl.w) %>% 
  mutate(value = as.numeric(value))

```

Now data are ready for resampling.  Use a different method for abundance data (resampling with replacement) and presence-absence data (random number generation from the binomial distribution).  Both methods produce a matrix named `zdata`.  

An extra step is required for data containing subsamples -- see below.

#### Abundance data

Use this method for abundance datasets which don't contain subsamples.

```{r Abundance, eval=FALSE}
res.ab <- function (z){(ddply (z, .(species), function(x) { 
  rs1 <- sample(x$value, replace = TRUE) # resample raw data
  bm <- mean(rs1) # big mean is the mean of the resampled means  
  data.frame(bm=bm)}
  ))
}

# Run 'res.ab' on each growth stage:
ar.r <- rdply (1000, res.ab (ar.l))
be.r <- rdply (1000, res.ab (be.l))
xx.r <- rdply (1000, res.ab (xx.l))
cm.r <- rdply (1000, res.ab (cm.l))
dl.r <- rdply (1000, res.ab (dl.l))

# Combine to a matrix:
zdata <- cbind (ar.r$.n, ar.r$bm, be.r$bm, xx.r$bm, cm.r$bm, dl.r$bm) # ar.r$.n = row labels
```

#### Presence-absence data

Use this method for presence-absence datasets which don't contain subsamples.

```{r Presence-absence, eval = FALSE}
res.pa <- function (z) {(ddply (z, .(species), function(x) { 
  mu <- mean (x$value)
  rn <- (rbinom (n = 1, size = length(x$value), prob = mu))/length(x$value) 
  # rn = random number based on mu
  data.frame(rn=rn)}
  ))
}

# Run 'res.pa' on each growth stage:
ar.r <- rdply (1000, res.pa (ar.l))
be.r <- rdply (1000, res.pa (be.l))
cm.r <- rdply (1000, res.pa (cm.l))
dl.r <- rdply (1000, res.pa (dl.l))

# Combine to a matrix:
zdata <- cbind (ar.r$.n, ar.r$rn, be.r$rn, cm.r$rn, dl.r$rn) # ar.r$.n = row labels
```

#### Alternative method for datasets containing subsamples.

Use the first method for abundance data and the second for presence-absence.  Edit `nrow` and `ncol` according to the number of sites (per GS) and the number of quadrats (per site).

```{r Subsamples: abundance data, eval = FALSE}
# Edit nrow and ncol according to numbers of subsamples/quadrats and sites
res.ab.s <- function (z) {(ddply (z, .(species), function(x) { 
  v <- matrix(x$value, nrow=7, ncol=3, byrow = FALSE) # nrow = quadrats, ncol = sites
  rs1 <- apply(v, 2, sample, replace = TRUE) # resample raw data
  mu1 <- apply (rs1, 2, mean) # take mean of each site's resampled data
  rs2 <- sample(mu1, replace=TRUE) # resample the means
  bm <- mean(rs2) # big mean is the mean of the resampled means  
  data.frame(bm=bm)}
  ))
}

# Run 'res.ab.s' on each growth stage:
ar.r <- rdply (1000, res.ab.s (ar.l))
be.r <- rdply (1000, res.ab.s (be.l))
cm.r <- rdply (1000, res.ab.s (cm.l))
dl.r <- rdply (1000, res.ab.s (dl.l))

# Combine to a matrix:
zdata <- cbind (ar.r$.n, ar.r$bm, be.r$bm, cm.r$bm, dl.r$bm) # ar.r$.n = row labels
```

```{r Subsamples: presence-absence data, eval = FALSE}
# Edit nrow and ncol according to numbers of subsamples/quadrats and sites
res.pa.s <- function (z) {(ddply (z, .(species), function(x) { 
  v <- matrix(x$value, nrow=10, ncol=3, byrow = FALSE) # nrow = quadrats, ncol = sites
  rb1 <- apply (v, 2, mean)
  mu <- rb1/length(rb1)
  rn <- (rbinom (n = 1, size = length(rb1), prob = mu))/length(rb1) # random number
  data.frame(rn=rn)}
  ))
}

# Run 'res.pa.s' on each growth stage:
ar.r <- rdply (1000, res.pa.s (ar.l))
be.r <- rdply (1000, res.pa.s (be.l))
cm.r <- rdply (1000, res.pa.s (cm.l))
dl.r <- rdply (1000, res.pa.s (dl.l))

# Then combine to a matrix:
zdata <- cbind (ar.r$.n, ar.r$rn, be.r$rn, cm.r$rn, dl.r$rn) # ar.r$.n = row labels
```


### Convert the matrix to a data frame and save it.

The same method applies to all data types from this point.

```{r Combine data, eval = FALSE}
mdata <- apply (zdata, 2, function (x) ifelse (x>0, x, 0.001)) 
# replace zeros for geometric mean calculation
rdata <- data.frame(mdata) # convert to a data frame
names(rdata) <- c("group", "AR", "BE", "XX", "CM", "DL")
rdata$group <- factor(rdata$group) # rdata is ready for the GSO function
```

***


## 2 Run the GSO function

The nloptr function maximises the geometric mean by changing the growth stage proportions given bounds constraints (parameters (GS proportions) must be between 0 and 1), and an inequality constraint -- the parameters must sum to 1. 

```{r GSO function, eval = FALSE, warning=FALSE}
library(nloptr)
library(plyr)
library(dplyr)
## Handy helper function, constructing a n-length vector c(k, ..., k)
const.vector <- function(n, k) {
  return( sapply(seq_len(n), function (x) return(k)) )
}
## The original objective (assumes columns [AR, BE, CM, DL] appear in that order) 
geomean.fun <- function(x, species) {
  logsum <- sum(log(apply(species, 1, function (row) { sum(x*row) })))
  exp(logsum/(dim(species)[1]))
}
## The reformulated objective (nloptr expects a minimization problem)
geomean.obj <- function (x, species) {
  logsum <- sum(log(apply(species, 1, function (row) { sum(x*row) })))
  return(-logsum)
}
## The gradient of geomean.obj.
## Returns a vector containing the partial derivatives wrt each var.
geomean.grad <- function(x, species) {
  contrib <- apply(species, 1, function(row) { -row/(sum(x*row)) })
  grad <- apply(contrib, 1, sum)  # sum the contribution vectors to get the gradient
  return(grad)
}
## Inequality constraints.
eval.cs <- function(x, species) {
  return( c(
    sum(x) - 1,  # Growth stage proportions don't exceed 1
    0.0 - x[1]   # Recently burnt proportion is at least 0.0 (i.e. AR >= 0.0)
  ))
}
eval.cs.jac <- function(x, species) { # Jacobian (partial derivative matrix) for inequalities
  return( rbind(
    const.vector(length(x), 1),
    c(-1, const.vector(length(x)-1, 0))
    ))
}
## Pass all to nloptr 
gso <- function(species){
  require(nloptr)
  require(dplyr)
  x0 <- const.vector(5, 1/5)  # edit according to the number of growth stages
  run <- nloptr(x0 = x0, species = species, eval_f=geomean.obj, eval_grad_f = geomean.grad,
                lb = const.vector(length(x0), 0),
                ub = const.vector(length(x0), 1),
                eval_g_ineq = eval.cs,
                eval_jac_g_ineq = eval.cs.jac,
                opts = list("algorithm" = "NLOPT_LD_MMA",
                            "maxeval"=1000,
                            "ftol_rel"=1.0e-15,
                            "xtol_rel"=1.0e-8,
                            "print_level"=0
                            ))
  res <- rbind(c(run$solution, geomean.fun(run$solution, species)))
  colnames(res) <- c("AR", "BE", "XX", "CM", "DL", "geom") # edit according to growth stages
  res <- data.frame(res)
  return (res)
}

# Apply to the 1000 resampled communities (defined by group)
out <- ddply(rdata[2:6], .(rdata$group), gso) # takes a few minutes to run

# Or apply to a single community (for example, the raw data)
#out <- gso (rdata)

write.table (out, "gso_result.txt", col.names=TRUE, row.names=FALSE)
```


```{r GSO function, eval = FALSE, warning=FALSE}
# BROKEN
library(nloptr)
library(plyr)
library(dplyr)
## Handy helper function, constructing a n-length vector c(k, ..., k)
const.vector <- function(n, k) {
  return(sapply(seq_len(n), function(x) return(k)))
}
## The original objective (assumes columns [AR, BE, CM, DL] appear in that order)
geomean.fun <- function(x, species) {
  logsum <- sum(log(apply(species, 1, function(row) {
    sum(x * row)
  })))
  exp(logsum / (dim(species)[1]))
}
## The reformulated objective (nloptr expects a minimization problem)
geomean.obj <- function(x, species) {
  logsum <- sum(log(apply(species, 1, function(row) {
    sum(x * row)
  })))
  return(-logsum)
}
## The gradient of geomean.obj.
## Returns a vector containing the partial derivatives wrt each var.
geomean.grad <- function(x, species) {
  contrib <- apply(species, 1, function(row) {
    -row / (sum(x * row))
  })
  grad <- apply(contrib, 1, sum) # sum the contribution vectors to get the gradient
  return(grad)
}
## Inequality constraints.
eval.cs <- function(x, species) {
  return(c(
    sum(x) - 1, # Growth stage proportions don't exceed 1
    0.0 - x[1] # Recently burnt proportion is at least 0.0 (i.e. AR >= 0.0)
  ))
}
eval.cs.jac <- function(x, species) { # Jacobian (partial derivative matrix) for inequalities
  return(rbind(
    const.vector(length(x), 1),
    c(-1, const.vector(length(x) - 1, 0))
  ))
}
## Pass all to nloptr
gso <- function(species) {
  require(nloptr)
  require(dplyr)
  x0 <- const.vector(4, 1 / 4) # edit according to the number of growth stages
  run <- nloptr(
    x0 = x0, species = species, eval_f = geomean.obj, eval_grad_f = geomean.grad,
    lb = const.vector(length(x0), 0),
    ub = const.vector(length(x0), 1),
    eval_g_ineq = eval.cs,
    eval_jac_g_ineq = eval.cs.jac,
    opts = list(
      "algorithm" = "NLOPT_LD_MMA",
      "maxeval" = 1000,
      "ftol_rel" = 1.0e-15,
      "xtol_rel" = 1.0e-8,
      "print_level" = 0
    )
  )
  res <- rbind(c(run$solution, geomean.fun(run$solution, species)))
  colnames(res) <- c("A", "B", "C", "D", "geom") # edit according to growth stages
  res <- data.frame(res)
  return(res)
}

# Apply to the 1000 resampled communities (defined by group)

out <- ddply(test[2:4], .(test$group), gso) # takes a few minutes to run

# Or apply to a single community (for example, the raw data)
out <- gso(test)

write.table(out, "gso_result.txt", col.names = TRUE, row.names = FALSE)
```

If `gso` is applied to 1000 resampled datasets, the object `out` contains 1000 results (optimal growth stage structures and geometric means).  The mean and 95% confidence intervals can be derived from this distribution of values. 

***
