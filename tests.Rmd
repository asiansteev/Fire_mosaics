---
title: "Test"
author: "Chris Adlam"
date: "7/20/2018"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Packages
```{r setup, include=FALSE}
if(!require('pacman'))install.packages('pacman')
pacman::p_load(tidyverse, emmeans, ggplot2, cowplot, vegan, data.table, kableExtra, plyr, nloptr, labdsv, bbmle, spdep, rsq,lmtest, unmarked, broom)

#library(dplyr)
#library(ggplot2)
library(purrr)
library(tibble)
#library(tidyr)
library(lmtest)
library(bbmle)
library(spdep)
library(rsq)
library(lmtest)
#options(contrasts =c("contr.sum", "contr.poly"))
#options(contrasts = rep ("contr.treatment", 2))

# load scripts
source("scripts/occ_plot.R")
```

# Bird data
```{r}
# read in bird data
bird_dat_count <- read.csv("data/bird_data.csv", header = T) %>% 
  filter(DetectionLocationNm != "O") %>% # removing species outside (O) the stand
  dplyr::select(Point, Count, Spp, DistanceBin) %>%  # keeping only relevant columns
  dplyr::rename(distance = DistanceBin)

# remove duplicate rows (same species detected multiple times in a single plot)
bird_dat_long <- bird_dat_count %>% 
  dplyr::rename(site_id = Point) %>% 
  dplyr::rename(species = Spp)
```

# Plant and site data
```{r include=F}
plant_data <- read.csv("/Users/christopheradlam/Desktop/Davis/R/GitHub Repos/Fire_mosaics/Data/plant_data.csv")
plant_names <- read.csv("/Users/christopheradlam/Desktop/Davis/R/GitHub Repos/Fire_mosaics/data/plant_list.csv")
plant_data$species <- as.character(plant_data$species)
plant_data$site_id <- as.character(plant_data$site_id)
plant_names$species <- as.character(plant_names$species)
plant_names$full_name <- as.character(plant_names$full_name)
plant_names$native_status <- as.factor(plant_names$native_status)
plant_names$form <- as.factor(plant_names$form)

# Keep only native species/ long format
plant_dat <- left_join(plant_data, plant_names, by = "species") %>%
  filter(native_status == "native") %>%
  dplyr::select(site_id, species, cover)

site_data <- read.csv("data/site_data.csv")

# Set tsf_cat in site_data NOT WORKING
site_data$tsf <- as.numeric(as.character(site_data$tsf))

site_data <- site_data %>% 
    mutate(tsf_cat2 = ifelse(is.na(tsf), "3", "2"))

site_data$tsf_cat2[site_data$tsf < 15] <-1


# Data prep
# convert to wide format for following analysis
plant_matrix1 <- spread(data = plant_dat, key = species, value = cover, fill = 0)

# Make Bray-Curtis dissimilarity matrix
plants_matrix <- as.matrix(plant_matrix1[, -1])
```

```{r get plant data}
#install.packages("fuzzySim") doesnt work, so here's the function; serves to convert from presence only to presence-absence

splist2presabs <- function(data, sites.col, sp.col, keep.n = FALSE) {
  # version 1.1 (7 May 2013)
  # data: a matrix or data frame with your localities and species (each in a different column)
  # sites.col: the name or index number of the column containing the localities
  # sp.col: the name or index number of the column containing the species names or codes
  # keep.n: logical, whether to get in the resulting table the number of times each species appears in each locality; if false (the default), only the presence (1) or absence (0) are recorded

  stopifnot(
    length(sites.col) == 1,
    length(sp.col) == 1,
    sites.col != sp.col,
    sites.col %in% 1 : ncol(data) | sites.col %in% names(data),
    sp.col %in% 1 : ncol(data) | sp.col %in% names(data),
    is.logical(keep.n)
  )

  presabs <- table(data[ , c(sites.col, sp.col)])
  presabs <- as.data.frame(unclass(presabs))
  if (!keep.n)  presabs[presabs > 1] <- 1
  presabs <- data.frame(row.names(presabs), presabs)
  names(presabs)[1] <- names(subset(data, select = sites.col))
  rownames(presabs) <- NULL
  return(presabs)
}  # end splist2presabs function

# executing function and going from wide to long:
plant_dat_pa <- splist2presabs(plant_dat, sites.col = 1, sp.col = 2) 

# data for plant glm using p/a
plant_glm_pa <- merge(plant_dat_pa, site_data, by= 'site_id') %>% 
  mutate(tsf_cat = as.factor(as.character(tsf_cat)))  %>% # if tsf_cat is numeric, model output is gibberish; must change to factor
  mutate(sev_tsf = paste(sev, tsf_cat2, sep = "-"))

plant_glm_pa$tsf_cat <- as.factor(plant_glm_pa$tsf_cat)

# For cover instead of p/a, use the following, which is the same as plant_mrpp_d:
site_data$site_id <- as.character(site_data$site_id) 
plant_glm_cov <- left_join(plant_matrix1, site_data, by = "site_id") %>% 
  mutate(sev_tsf = paste(sev, tsf_cat2, sep = "-"))

plant_glm_cov$tsf_cat <- as.factor(plant_glm_cov$tsf_cat)
```

# Cluster analysis 1
```{r}
# Cluster Analysis
mydata <- 
str(mydata)
head(mydata)
pairs(mydata)

# Scatter plot 
plot(mydata$Fuel_Cost~ mydata$Sales, data = mydata)
with(mydata,text(mydata$Fuel_Cost ~ mydata$Sales, labels=mydata$Company,pos=4))

# Normalize 
z = mydata[,-c(1,1)]
means = apply(z,2,mean)
sds = apply(z,2,sd)
nor = scale(z,center=means,scale=sds)

##calculate distance matrix (default is Euclidean distance)
distance = dist(nor)

# Hierarchical agglomerative clustering using default complete linkage 
mydata.hclust = hclust(distance)
plot(mydata.hclust)
plot(mydata.hclust,labels=mydata$Company,main='Default from hclust')
plot(mydata.hclust,hang=-1)

# Hierarchical agglomerative clustering using "average" linkage 
mydata.hclust<-hclust(distance,method="average")
plot(mydata.hclust,hang=-1)

# Cluster membership
member = cutree(mydata.hclust,3)
table(member)

#Characterizing clusters 
aggregate(nor,list(member),mean)
aggregate(mydata[,-c(1,1)],list(member),mean)

# Scree Plot
wss <- (nrow(nor)-1)*sum(apply(nor,2,var))
for (i in 2:20) wss[i] <- sum(kmeans(nor, centers=i)$withinss)
plot(1:20, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares") 

# K-means clustering
kc<-kmeans(nor,3)
```


# Cluster analysis 2
```{r}
# the cluster analysis is to figure out the TSF thresholds; also at what point are UN and LS site indistinguishable? how about UN/LS and HS?


#install.packages(c("cluster", "factoextra"))
library(cluster)
library(factoextra)

df <- plant_glm_cov[, c(1:139)]

df <- subset(df, select = -c(CESA, ALRH, SALU))

df1 <- df[,-1]
rownames(df1) <- df[,1]

df.scaled <- scale(df1)

#trying spearman correlation but returns all NAs
dist_corr <- get_dist(df.scaled, stand = TRUE, method = "spearman")

#Euclidean dist instead
dist.eucl <- dist(df.scaled, method = "euclidean")
#fviz_dist(dist.eucl) # visualization

# for some reason this wouldn't work until I ran the fire_mosaics.Rmd....
# But when scaled it just suggests 1 cluster vs 6 when not clustered
mydata <- as.matrix(df1)
fviz_nbclust(mydata, kmeans, method = "gap_stat")

km.res <- kmeans(mydata, 4) #, nstart = 25)
#which(apply(mydata, 2, var)==0) # to find columns with 0 variance

# Visualize
fviz_cluster(km.res, data = mydata, palette = "jco",
             ggtheme = theme_minimal())

```
# Cluster analysis 2 TSF
```{r}
# the cluster analysis is to figure out the TSF thresholds; also at what point are UN and LS site indistinguishable? how about UN/LS and HS?


#install.packages(c("cluster", "factoextra"))
library(cluster)
library(factoextra)

plant_glm_cov1 <- plant_glm_cov %>% 
  filter(sev== "l")

df <- plant_glm_cov1[, c(1:139)] 

df1 <- df[,-1]
rownames(df1) <- df[,1]

nosp <- as.data.frame(which(apply(df1, 2, var) == 0)) # to find columns with 0 variance
nosp <- cbind(rownames(nosp), data.frame(nosp, row.names=NULL)) # change row names to column

# remove spp not in lS
df2 <- subset(df1, select = -c(nosp[,1]))

#df1 <- df[,-1]
#rownames(df1) <- df[,1]

df.scaled <- scale(df2)

#trying spearman correlation but returns all NAs
dist_corr <- get_dist(df.scaled, stand = TRUE, method = "spearman")

#Euclidean dist instead
dist.eucl <- dist(df.scaled, method = "euclidean")
#fviz_dist(dist.eucl) # visualization

# for some reason this wouldn't work until I ran the fire_mosaics.Rmd....
# But when scaled it just suggests 1 cluster vs 6 when not clustered
mydata <- as.matrix(df.scaled)
fviz_nbclust(mydata, kmeans, method = "gap_stat")

km.res <- kmeans(mydata, 3) #, nstart = 25)

# Visualize
fviz_cluster(km.res, data = mydata, palette = "jco",
             ggtheme = theme_minimal())

```

# Occ_plot function (cov)
```{r}
# how to transform skewed data?
# check if there is a nice bell curve or if it's truncated (gaussian not so good if skewed)
# transform SE to standard dev

# to extract the coefficients:
#coef(summary(CEIN_mod))["(Intercept)","Estimate"]

CEIN_tidy <- tidy(CEIN_mod)

# equivalently using package broom:
## construct function; this works with sev and tsf_cat (3 categories)
occ_plot_full <- function(mod) {
CEIN_tidy <- tidy(mod)
# I think there should only be 2 age categories for ls and hs rather than 3
hs1_mean <- CEIN_tidy[1,2]
hs2_mean <- CEIN_tidy[1,2] + CEIN_tidy[4,2]
hs3_mean <- CEIN_tidy[1,2] + CEIN_tidy[5,2]
ls1_mean <- CEIN_tidy[1,2] + CEIN_tidy[2,2]
ls2_mean <- CEIN_tidy[1,2] + CEIN_tidy[2,2] + CEIN_tidy[4,2]
ls3_mean <- CEIN_tidy[1,2] + CEIN_tidy[2,2] + CEIN_tidy[5,2]
un_mean <- CEIN_tidy[1,2] + CEIN_tidy[3,2]

mean <- unlist(c(hs1_mean, hs2_mean, hs3_mean, ls1_mean, ls2_mean, ls3_mean, un_mean))

hs1_SE <- CEIN_tidy[1,3]
hs2_SE <- CEIN_tidy[1,3] + CEIN_tidy[4,3]
hs3_SE <- CEIN_tidy[1,3] + CEIN_tidy[5,3]
ls1_SE <- CEIN_tidy[1,3] + CEIN_tidy[2,3]
ls2_SE <- CEIN_tidy[1,3] + CEIN_tidy[2,3] + CEIN_tidy[4,3]
ls3_SE <- CEIN_tidy[1,3] + CEIN_tidy[2,3] + CEIN_tidy[5,3]
un_SE <- CEIN_tidy[1,3] + CEIN_tidy[3,3]

SE <- unlist(c(hs1_SE, hs2_SE, hs3_SE, ls1_SE, ls2_SE, ls3_SE, un_SE))

hab <- c("hs1", "hs2", "hs3", "ls1", "ls2", "ls3", 'un')


plot_data <- data.frame(hab, mean, SE)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-SE, ymax=mean+SE), width=.2,
                 position=position_dodge(.9))

  return(p)
}


##THis works with sev_tsf
occ_plot_sevtsf <- function(mod) {
CEIN_tidy <- tidy(mod)
# I think there should only be 2 age categories for ls and hs rather than 3
hs1_mean <- CEIN_tidy[1,2]
hs2_mean <- CEIN_tidy[1,2] + CEIN_tidy[2,2]
hs3_mean <- CEIN_tidy[1,2] + CEIN_tidy[3,2]
ls1_mean <- CEIN_tidy[1,2] + CEIN_tidy[4,2]
ls2_mean <- CEIN_tidy[1,2] + CEIN_tidy[5,2]
ls3_mean <- CEIN_tidy[1,2] + CEIN_tidy[6,2]
un_mean <- CEIN_tidy[1,2] + CEIN_tidy[7,2]

mean <- unlist(c(hs1_mean, hs2_mean, hs3_mean, ls1_mean, ls2_mean, ls3_mean, un_mean))

hs1_SE <- CEIN_tidy[1,3]
hs2_SE <- CEIN_tidy[1,3] + CEIN_tidy[2,3]
hs3_SE <- CEIN_tidy[1,3] + CEIN_tidy[3,3]
ls1_SE <- CEIN_tidy[1,3] + CEIN_tidy[4,3]
ls2_SE <- CEIN_tidy[1,3] + CEIN_tidy[5,3]
ls3_SE <- CEIN_tidy[1,3] + CEIN_tidy[6,3]
un_SE <- CEIN_tidy[1,3] + CEIN_tidy[7,3]

SE <- unlist(c(hs1_SE, hs2_SE, hs3_SE, ls1_SE, ls2_SE, ls3_SE, un_SE))

hab <- c("hs1", "hs2", "hs3", "ls1", "ls2", "ls3", 'un')


plot_data <- data.frame(hab, mean, SE)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-SE, ymax=mean+SE), width=.2,
                 position=position_dodge(.9))

  return(p)
}

#sev only
occ_plot_sev <- function(mod) {
CEIN_tidy <- tidy(mod)
# I think there should only be 2 age categories for ls and hs rather than 3
hs_mean <- CEIN_tidy[1,2]
ls_mean <- CEIN_tidy[1,2] + CEIN_tidy[2,2]
un_mean <- CEIN_tidy[1,2] + CEIN_tidy[3,2]

mean <- unlist(c(hs_mean, ls_mean, un_mean))

hs_SE <- CEIN_tidy[1,3]
ls_SE <- CEIN_tidy[1,3] + CEIN_tidy[2,3]
un_SE <- CEIN_tidy[1,3] + CEIN_tidy[3,3]

SE <- unlist(c(hs_SE, ls_SE, un_SE))

hab <- c("HS", "LS", "UN")


plot_data <- data.frame(hab, mean, SE)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-SE, ymax=mean+SE), width=.2,
                 position=position_dodge(.9))

  return(p)
}

#tsf_cat only (3 categories plus UN)
occ_plot_tsf <- function(mod) {
CEIN_tidy <- tidy(mod)
# I think there should only be 2 age categories for ls and hs rather than 3
mean1 <- CEIN_tidy[1,2]
mean2 <- CEIN_tidy[1,2] + CEIN_tidy[2,2]
mean3 <- CEIN_tidy[1,2] + CEIN_tidy[3,2]
un_mean <- CEIN_tidy[1,2] + CEIN_tidy[4,2]

mean <- unlist(c(mean1, mean2, mean3, un_mean))

SE1 <- CEIN_tidy[1,3]
SE2 <- CEIN_tidy[1,3] + CEIN_tidy[2,3]
SE3 <- CEIN_tidy[1,3] + CEIN_tidy[3,3]
un_SE <- CEIN_tidy[1,3] + CEIN_tidy[4,3]

SE <- unlist(c(SE1, SE2, SE3, un_SE))

hab <- c("1", "2", "3", 'un')


plot_data <- data.frame(hab, mean, SE)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-SE, ymax=mean+SE), width=.2,
                 position=position_dodge(.9))

  return(p)
}

#full with 2 tsf categories
occ_plot_full2cat <- function(mod) {
CEIN_tidy <- tidy(mod)
# I think there should only be 2 age categories for ls and hs rather than 3
hs1_mean <- CEIN_tidy[1,2]
hs2_mean <- CEIN_tidy[1,2] + CEIN_tidy[4,2]
ls1_mean <- CEIN_tidy[1,2] + CEIN_tidy[2,2]
ls2_mean <- CEIN_tidy[1,2] + CEIN_tidy[2,2] + CEIN_tidy[4,2]
un_mean <- CEIN_tidy[1,2] + CEIN_tidy[3,2]

mean <- unlist(c(hs1_mean, hs2_mean, ls1_mean, ls2_mean, un_mean))

hs1_SE <- CEIN_tidy[1,3]
hs2_SE <- CEIN_tidy[1,3] + CEIN_tidy[4,3]
ls1_SE <- CEIN_tidy[1,3] + CEIN_tidy[2,3]
ls2_SE <- CEIN_tidy[1,3] + CEIN_tidy[2,3] + CEIN_tidy[4,3]
un_SE <- CEIN_tidy[1,3] + CEIN_tidy[3,3]

SE <- unlist(c(hs1_SE, hs2_SE, ls1_SE, ls2_SE, un_SE))

hab <- c("hs1", "hs2", "ls1", "ls2", 'un')


plot_data <- data.frame(hab, mean, SE)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-SE, ymax=mean+SE), width=.2,
                 position=position_dodge(.9))

  return(p)
}

```

# Occ_plot function for p/a
```{r}
## construct function; this works with sev and tsf_cat (3 categories)
#mod <- sp_mod1

#turns out the full, multiplicative model is never the best!
occ_plot_full <- function(mod) {
CEIN_tidy <- tidy(mod)
# I think there should only be 2 age categories for ls and hs rather than 3
hs1_mean <- logit2prob(CEIN_tidy[1,2])
hs2_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[4,2])
hs3_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[5,2])
ls1_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[2,2])
ls2_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[2,2]) + logit2prob(CEIN_tidy[4,2])
ls3_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[2,2]) + logit2prob(CEIN_tidy[5,2])
un_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[3,2])

mean <- unlist(c(hs1_mean, hs2_mean, hs3_mean, ls1_mean, ls2_mean, ls3_mean, un_mean))

hs1_SD <- sqrt(logit2prob(CEIN_tidy[1,3]))
hs2_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[4,3]))
hs3_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[5,3]))
ls1_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[2,3]))
ls2_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[2,3])) + sqrt(logit2prob(CEIN_tidy[4,3]))
ls3_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[2,3])) + sqrt(logit2prob(CEIN_tidy[5,3]))
un_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[3,3]))

SD <- unlist(c(hs1_SD, hs2_SD, hs3_SD, ls1_SD, ls2_SD, ls3_SD, un_SD))

hab <- c("hs1", "hs2", "hs3", "ls1", "ls2", "ls3", 'un')


plot_data <- data.frame(hab, mean, SD)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-SD, ymax=mean+SD), width=.2,
                 position=position_dodge(.9))

  return(p)
}

# I think there should only be 2 age categories for ls and hs rather than 3
## This works with sev_tsf
occ_plot_sevtsf <- function(mod) {
CEIN_tidy <- tidy(mod)
# I think there should only be 2 age categories for ls and hs rather than 3
hs1_mean <- logit2prob(CEIN_tidy[1,2])
hs2_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[2,2])
hs3_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[3,2])
ls1_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[4,2])
ls2_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[5,2])
ls3_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[6,2])
un_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[7,2])

mean <- unlist(c(hs1_mean, hs2_mean, hs3_mean, ls1_mean, ls2_mean, ls3_mean, un_mean))

hs1_SD <- sqrt(logit2prob(CEIN_tidy[1,3]))
hs2_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[2,3]))
hs3_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[3,3]))
ls1_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[4,3]))
ls2_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[5,3]))
ls3_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[6,3]))
un_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[7,3]))

SD <- unlist(c(hs1_SD, hs2_SD, hs3_SD, ls1_SD, ls2_SD, ls3_SD, un_SD))

hab <- c("hs1", "hs2", "hs3", "ls1", "ls2", "ls3", 'un')


plot_data <- data.frame(hab, mean, SD)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-SD, ymax=mean+SD), width=.2,
                 position=position_dodge(.9))

  return(p)
}

# sev only/ NOT WORKING; maybe something with the link function, or the logit2prob function



occ_plot_sev <- function(mod) {
# This works nicely but couldn't extract the values to make a graph
#A <- allEffects(sp_mod1)
#summary(A)
# OR 
#X <- predictorEffects((sp_mod1))
#see also ?confint

#hs
predicted = predict(mod, data.frame(sev='h'), type='link', se.fit=TRUE)

hs_mean = logit2prob(predicted$fit)
hs_CI_high = logit2prob(predicted$fit + (predicted$se.fit*1.96)) - hs_mean 
hs_CI_low = hs_mean - logit2prob(predicted$fit - (predicted$se.fit*1.96))

#ls
predicted = predict(mod, data.frame(sev='l'), type='link', se.fit=TRUE)
ls_mean = logit2prob(predicted$fit)
ls_CI_high = logit2prob(predicted$fit + (predicted$se.fit*1.96)) - ls_mean 
ls_CI_low = ls_mean - logit2prob(predicted$fit - (predicted$se.fit*1.96))

#ls
predicted = predict(mod, data.frame(sev='u'), type='link', se.fit=TRUE)
un_mean = logit2prob(predicted$fit)
un_CI_high = logit2prob(predicted$fit + (predicted$se.fit*1.96)) - un_mean 
un_CI_low = un_mean -logit2prob(predicted$fit - (predicted$se.fit*1.96))

mean <- unlist(c(hs_mean, ls_mean, un_mean))
CI_high <- unlist(c(hs_CI_high, ls_CI_high, un_CI_high))
CI_low <- unlist(c(hs_CI_low, ls_CI_low, un_CI_low))

hab <- c("HS", "LS", "UN")

plot_data <- data.frame(hab, mean, CI_high, CI_low)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-CI_low, ymax=mean+CI_high), width=.2,
                 position=position_dodge(.9))

  return(p)
}

#tsf_cat only (3 categories plus UN)
occ_plot_tsf <- function(mod) {
CEIN_tidy <- tidy(mod)
# I think there should only be 2 age categories for ls and hs rather than 3
mean1 <- logit2prob(CEIN_tidy[1,2])
mean2 <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[2,2])
mean3 <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[3,2])
un_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[4,2])

mean <- unlist(c(mean1, mean2, mean3, un_mean))

SD1 <- sqrt(logit2prob(CEIN_tidy[1,3]))
SD2 <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[2,3]))
SD3 <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[3,3]))
un_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[4,3]))

SD <- unlist(c(SD1, SD2, SD3, un_SD))

hab <- c("1", "2", "3", 'un')


plot_data <- data.frame(hab, mean, SD)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-SD, ymax=mean+SD), width=.2,
                 position=position_dodge(.9))

  return(p)
}

#full with 2 tsf categories
occ_plot_full2cat <- function(mod) {
CEIN_tidy <- tidy(mod)
# I think there should only be 2 age categories for ls and hs rather than 3
hs1_mean <- logit2prob(CEIN_tidy[1,2])
hs2_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[4,2])
ls1_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[2,2])
ls2_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[2,2]) + logit2prob(CEIN_tidy[4,2])
un_mean <- logit2prob(CEIN_tidy[1,2]) + logit2prob(CEIN_tidy[3,2])

mean <- unlist(c(hs1_mean, hs2_mean, ls1_mean, ls2_mean, un_mean))

hs1_SD <- sqrt(logit2prob(CEIN_tidy[1,3]))
hs2_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[4,3]))
ls1_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[2,3]))
ls2_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[2,3])) + sqrt(logit2prob(CEIN_tidy[4,3]))
un_SD <- sqrt(logit2prob(CEIN_tidy[1,3])) + sqrt(logit2prob(CEIN_tidy[3,3]))

SD <- unlist(c(hs1_SD, hs2_SD, ls1_SD, ls2_SD, un_SD))

hab <- c("hs1", "hs2", "ls1", "ls2", 'un')


plot_data <- data.frame(hab, mean, SD)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-SD, ymax=mean+SD), width=.2,
                 position=position_dodge(.9))

  return(p)
}


#tsf_cat only (2 categories plus UN)
occ_plot_tsf_cat2 <- function(mod) {
  
# 1
predicted = predict(mod, data.frame(tsf_cat2='1'), type='link', se.fit=TRUE)

mean1 = logit2prob(predicted$fit)
CI_high1 = logit2prob(predicted$fit + (predicted$se.fit*1.96)) - mean1
CI_low1 = mean1 - logit2prob(predicted$fit - (predicted$se.fit*1.96))

# 2
predicted = predict(mod, data.frame(tsf_cat2='2'), type='link', se.fit=TRUE)
mean2 = logit2prob(predicted$fit)
CI_high2 = logit2prob(predicted$fit + (predicted$se.fit*1.96)) - mean2 
CI_low2 = mean2 - logit2prob(predicted$fit - (predicted$se.fit*1.96))

# 3
predicted = predict(mod, data.frame(tsf_cat2='3'), type='link', se.fit=TRUE)
mean3 = logit2prob(predicted$fit)
CI_high3 = logit2prob(predicted$fit + (predicted$se.fit*1.96)) - mean3 
CI_low3 = mean3 -logit2prob(predicted$fit - (predicted$se.fit*1.96))

mean <- unlist(c(mean1, mean2, mean3))
CI_high <- unlist(c(CI_high1, CI_high2, CI_high3))
CI_low <- unlist(c(CI_low1, CI_low2, CI_low3))

hab <- c("0-15", "15-31", "UN")

plot_data <- data.frame(hab, mean, CI_high, CI_low)

p <- ggplot(plot_data, aes(x = hab, y = mean)) +
  geom_col() +
  geom_errorbar(aes(ymin=mean-CI_low, ymax=mean+CI_high), width=.2,
                 position=position_dodge(.9))

  return(p)
}
```


# Frank's goodness of fit code (not working)
```{r}
#Goodness-of-fit tests
#This runs three GOF tests: Chi square, Freeman Tukey, and SSE. What you're basically looking for is that your data doesn't fall near the extremes of the bootstrapped distribution. A good threshold for each statistic might be >.1 and <0.9. Probably good to run at least 100 simulations, more is preferable (nsim in the bottom code). These can be slow.

#m12 is your model fit - if this doesn't work, you made need to play around with the model output you are using and figure out how to extract the required elements below
fm <- m12

fm <- CEIN_mod

str(CEIN_mod)
#Function for the GOF tests - if this doesn't work, check your model output and make sure observed, expected, and residuals are being properly extracted and amend those first three lines as needed. I haven't tried this with a GLM output, but it should be an easy fix if it doesn't work out of the box.
fitstats <- function(fm) {
  observed <- fm$data
  expected <- fm$fitted.values
  resids <- fm$residuals
  sse <- sum(resids^2)
  chisq <- sum((observed - expected)^2 / expected)
  freeTuke <- sum((sqrt(observed) - sqrt(expected))^2)
  out <- c(SSE=sse, Chisq=chisq, freemanTukey=freeTuke)
  return(out)
}

fitstats <- function(fm) {
  observed <- fm$data
  expected <- fm$fitted.values
  resids <- fm$residuals
  sse <- sum(resids^2)
  chisq <- sum((observed - expected)^2 / expected)
  freeTuke <- sum((sqrt(observed) - sqrt(expected))^2)
  out <- c(SSE=sse, Chisq=chisq, freemanTukey=freeTuke)
  return(out)
}


#Bootstrapping the GOF - this is using parboot() from Unmarked. It may not play nicely with GLM output. If not, try the boot() function from the boot package in place of parboot(). I think the syntax is nearly identical except maybe the 'report' part which isn't important.
(pb <- parboot(fm, fitstats, nsim=100, report=1))

library(boot)
pb <- boot(fm, fitstats, R=100)

?boot

#Plot isn't necessary but might help you understand the outputs - may need some tweeking to work for boot()
plot(pb, main="")
```


```{r Moran's I}
#library(ape)

dists <- as.matrix(dist(cbind(plant_glm_cov$lon, plant_glm_cov$lat)))
#dists.inv <- 1/dists
#diag(dists.inv) <- 0
#Moran.I(plant_mrpp_d$PSME, dists.inv, na.rm = T)

# convert w to a row standardised general weights object
lw <- mat2listw(dists)
lwW <- nb2listw(lw$neighbours, glist=lw$weights, style="W")
CEIN_mi <- (moran.test(plant_glm_cov$MECA, lwW, alternative="two.sided"))$statistic

#CEIN_mi <- (moran.test(MECA_dat[,2], lwW, alternative="two.sided"))$statistic

CEIN_mip <- (moran.test(plant_glm_cov$MECA, lwW, alternative="two.sided"))$'p.value'
```

```{r AIC test}
d <- as.data.frame(UCBAdmissions)
d <- tidyr::spread(d, Admit, Freq) # use Hadley's excellent tidyr to reshape
d[order(d$Dept), ]

m1 <- glm(cbind(Admitted, Rejected) ~ Gender, d, family='binomial')
m2 <- glm(cbind(Admitted, Rejected) ~ Dept, d, family = 'binomial')
m3 <- glm(cbind(Admitted, Rejected) ~ Dept + Gender, d, family = 'binomial')
model.names <- c("1 Gender", "2 Dept", "3 Gender + Dept")

summ.table <- do.call(rbind, lapply(list(m1, m2, m3), broom::glance))

table.cols <- c("df.residual", "deviance", "AIC")
reported.table <- summ.table[table.cols]
names(reported.table) <- c("Resid. Df", "Resid. Dev", "AIC")

reported.table[['dAIC']] <-  with(reported.table, AIC - min(AIC))
reported.table[['weight']] <- with(reported.table, exp(- 0.5 * dAIC) / sum(exp(- 0.5 * dAIC)))
reported.table$AIC <- NULL
reported.table$weight <- round(reported.table$weight, 2)
reported.table$dAIC <- round(reported.table$dAIC, 1)
row.names(reported.table) <- model.names

```

# lmt function (p/a)
```{r lmt function}
#Converting logit scale to natural scale. The link function for binomial is logit, so you need to backtransform to get from that scale to a probability of occurence (0-1). Use the following function. Be sure to add together any coefficients and intercepts first - all arithmetic should be done before backtransformation:

logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

#If you do anything that uses Poisson, that's on the log scale so you can just use exp() to convert to the natural scale.


# before the function can add a line to the table, need to create the table!
tbl_headr <- c("sp", "Model", "dAICc", "w", "X2", "df", "p", "R2", 'Moran I', 'Moran I (p)')

#function:
lmt <- function(sp_dat) {
tsf <- glm(sp_dat[,2] ~ tsf_cat2, sp_dat, family = binomial) #sp cover /100 because it has to be between 0 and 1
sev <- glm(sp_dat[,2] ~ sev, sp_dat, family = binomial)
sp_null  <-glm(sp_dat[,2] ~ 1, sp_dat, family = binomial)
add <- glm(sp_dat[,2] ~ sev + tsf_cat2, sp_dat, family = binomial)
mult <- glm(sp_dat[,2] ~ sev * tsf_cat2, sp_dat, family = binomial)
null <- glm(sp_dat[,2] ~ 1, sp_dat, family = binomial)
  
# AIC and likelihood ratio test
sp_aic <- AICctab(null, add, mult, sev, tsf, base=T, delta=T, weights=T)

# R2 values 

# row 1 R2
if(attr(sp_aic, "row.names")[1] == "sev") {
  sp_R2_1 <- rsq(sev, adj = T)
} else if(attr(sp_aic, "row.names")[1] == "tsf") {
  sp_R2_1 <- rsq(tsf, adj = T)
} else if(attr(sp_aic, "row.names")[1] == "add") {
  sp_R2_1 <- rsq(add, adj = T)
} else if(attr(sp_aic, "row.names")[1] == "null") {
  sp_R2_1 <- rsq(null, adj = T)
} else {
  sp_R2_1 <- rsq(mult, adj = T)
}

# row 2 R2
if(attr(sp_aic, "row.names")[2] == "sev") {
  sp_R2_2 <- rsq(sev, adj = T)
} else if(attr(sp_aic, "row.names")[2] == "tsf") {
  sp_R2_2 <- rsq(tsf, adj = T)
} else if(attr(sp_aic, "row.names")[2] == "add") {
  sp_R2_2 <- rsq(add, adj = T)
} else if(attr(sp_aic, "row.names")[2] == "null") {
  sp_R2_2 <- rsq(null, adj = T)
} else {
  sp_R2_2 <- rsq(mult, adj = T)
}

# row 3 R2
if(attr(sp_aic, "row.names")[3] == "sev") {
  sp_R2_3 <- rsq(sev, adj = T)
} else if(attr(sp_aic, "row.names")[3] == "tsf") {
  sp_R2_3 <- rsq(tsf, adj = T)
} else if(attr(sp_aic, "row.names")[3] == "add") { 
  sp_R2_3 <- rsq(add, adj = T)
} else if(attr(sp_aic, "row.names")[3] == "null") {
  sp_R2_3 <- rsq(null, adj = T)
} else {
  sp_R2_3 <- rsq(mult, adj = T)
}

# row 4 R2
if(attr(sp_aic, "row.names")[4] == "sev") {
  sp_R2_4 <- rsq(sev, adj = T)
} else if(attr(sp_aic, "row.names")[4] == "tsf") {
  sp_R2_4 <- rsq(tsf, adj = T)
} else if(attr(sp_aic, "row.names")[4] == "add") {
  sp_R2_4 <- rsq(add, adj = T)
} else if(attr(sp_aic, "row.names")[4] == "null") {
  sp_R2_4 <- rsq(null, adj = T)
} else {
  sp_R2_4 <- rsq(mult, adj = T)
}

# row 5 R2
if(attr(sp_aic, "row.names")[5] == "sev") {
  sp_R2_5 <- rsq(sev, adj = T)
} else if(attr(sp_aic, "row.names")[5] == "tsf") {
  sp_R2_5 <- rsq(tsf, adj = T)
} else if(attr(sp_aic, "row.names")[5] == "add") {
  sp_R2_5 <- rsq(add, adj = T)
} else if(attr(sp_aic, "row.names")[5] == "null") {
  sp_R2_5 <- rsq(null, adj = T)
} else {
  sp_R2_5 <- rsq(mult, adj = T)
}

# Likelihood Ratio Test
# row 1 R2
if(attr(sp_aic, "row.names")[1] == "sev") {
  sp_lrt_1 <- lrtest(sev, sp_null)
} else if(attr(sp_aic, "row.names")[1] == "tsf") {
  sp_lrt_1 <- lrtest(tsf, sp_null)
} else if(attr(sp_aic, "row.names")[1] == "add") {
  sp_lrt_1 <- lrtest(add, sp_null)
} else if(attr(sp_aic, "row.names")[1] == "null") {
  sp_lrt_1 <- lrtest(null, sp_null)
} else {
  sp_lrt_1 <- lrtest(mult, sp_null)
}

# row 2 R2
if(attr(sp_aic, "row.names")[2] == "sev") {
  sp_lrt_2 <- lrtest(sev, sp_null)
} else if(attr(sp_aic, "row.names")[2] == "tsf") {
  sp_lrt_2 <- lrtest(tsf, sp_null)
} else if(attr(sp_aic, "row.names")[2] == "add") {
  sp_lrt_2 <- lrtest(add, sp_null)
} else if(attr(sp_aic, "row.names")[2] == "null") {
  sp_lrt_2 <- lrtest(null, sp_null)
} else {
  sp_lrt_2 <- lrtest(mult, sp_null)
}

# row 3 R2
if(attr(sp_aic, "row.names")[3] == "sev") {
  sp_lrt_3 <- lrtest(sev, sp_null)
} else if(attr(sp_aic, "row.names")[3] == "tsf") {
  sp_lrt_3 <- lrtest(tsf, sp_null)
} else if(attr(sp_aic, "row.names")[3] == "add") {
  sp_lrt_3 <- lrtest(add, sp_null)
} else if(attr(sp_aic, "row.names")[3] == "null") {
  sp_lrt_3 <- lrtest(null, sp_null)
} else {
  sp_lrt_3 <- lrtest(mult, sp_null)
}

# row 4 R2
if(attr(sp_aic, "row.names")[4] == "sev") {
  sp_lrt_4 <- lrtest(sev, sp_null)
} else if(attr(sp_aic, "row.names")[4] == "tsf") {
  sp_lrt_4 <- lrtest(tsf, sp_null)
} else if(attr(sp_aic, "row.names")[4] == "add") {
  sp_lrt_4 <- lrtest(add, sp_null)
} else if(attr(sp_aic, "row.names")[4] == "null") {
  sp_lrt_4 <- lrtest(null, sp_null)
} else {
  sp_lrt_4 <- lrtest(mult, sp_null)
}

# row 5 R2
if(attr(sp_aic, "row.names")[5] == "sev") {
  sp_lrt_5 <- lrtest(sev, sp_null)
} else if(attr(sp_aic, "row.names")[5] == "tsf") {
  sp_lrt_5 <- lrtest(tsf, sp_null)
} else if(attr(sp_aic, "row.names")[5] == "add") {
  sp_lrt_5 <- lrtest(add, sp_null)
} else if(attr(sp_aic, "row.names")[5] == "null") {
  sp_lrt_5 <- lrtest(null, sp_null)
} else {
    sp_lrt_5 <- lrtest(mult, sp_null)
}


#sp_lrt_full <- lrtest(full, sp_null)
#sp_lrt_sev <- lrtest(sev, sp_null)
#sp_lrt_tsfcat <- lrtest(tsf, sp_null)

#moran's I (first load function)
sp_mi <- (moran.test(sp_dat[,2], lwW, alternative="two.sided"))$statistic
sp_mip <- (moran.test(sp_dat[,2], lwW, alternative="two.sided"))$'p.value'

# construct table
sp_tbl1 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[1], sp_aic$dAICc[1], sp_aic$weight[1], sp_lrt_1$Chisq[2], sp_lrt_1$'#Df'[1], sp_lrt_1$'Pr(>Chisq)'[2], sp_R2_1, sp_mi, sp_mip)

sp_tbl2 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[2], sp_aic$dAICc[2], sp_aic$weight[2], sp_lrt_2$Chisq[2], sp_lrt_2$'#Df'[1], sp_lrt_2$'Pr(>Chisq)'[2], sp_R2_2, sp_mi, sp_mip)

sp_tbl3 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[3], sp_aic$dAICc[3], sp_aic$weight[3], sp_lrt_3$Chisq[2], sp_lrt_3$'#Df'[1], sp_lrt_3$'Pr(>Chisq)'[2], sp_R2_3, sp_mi, sp_mip)

sp_tbl4 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[4], sp_aic$dAICc[4], sp_aic$weight[4], sp_lrt_4$Chisq[2], sp_lrt_4$'#Df'[1], sp_lrt_4$'Pr(>Chisq)'[2], sp_R2_4, sp_mi, sp_mip)

sp_tbl5 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[5], sp_aic$dAICc[5], sp_aic$weight[5], sp_lrt_5$Chisq[2], sp_lrt_5$'#Df'[1], sp_lrt_5$'Pr(>Chisq)'[2], sp_R2_5, sp_mi, sp_mip)

tbl <- data.frame(rbind(tbl_headr, sp_tbl1, sp_tbl2, sp_tbl3, sp_tbl4, sp_tbl5)) 

# Turn first row into column names
names(tbl) <- lapply(tbl[1, ], as.character)
tbl <- tbl[-1,]

# convert to numeric
tbl[,c(3:10)] <- as.numeric(as.character(unlist(tbl[,c(3:10)]))) 

#remove row names
rownames(tbl) <- c()

tbl
}
```

# lmt function (cov)
```{r}
#need to fix if else like above
#function:
lmt <- function(sp_dat) {
tsf <- glm(sp_dat[,2]/100 ~ tsf_cat, sp_dat, family = gaussian) #sp cover /100 because it has to be between 0 and 1
sev <- glm(sp_dat[,2]/100 ~ sev, sp_dat, family = gaussian)
sp_null  <-glm(sp_dat[,2]/100 ~ 1, sp_dat, family = gaussian)
add <- glm(sp_dat[,2]/100 ~ sev + tsf_cat, sp_dat, family = gaussian)
mult <- glm(sp_dat[,2]/100 ~ sev * tsf_cat, sp_dat, family = gaussian)
null <- glm(sp_dat[,2]/100 ~ 1, sp_dat, family = gaussian)
  
# AIC and likelihood ratio test
sp_aic <- AICctab(null, add, mult, sev, tsf, base=T, delta=T, weights=T)

# R2 values 

# row 1 R2
if(attr(sp_aic, "row.names")[1] == "sev")
    {sp_R2_1 <- rsq(sev, adj = T)}
if(attr(sp_aic, "row.names")[1] == "tsf")
    {sp_R2_1 <- rsq(tsf, adj = T)}
if(attr(sp_aic, "row.names")[1] == "add")
    {sp_R2_1 <- rsq(add, adj = T)}
if(attr(sp_aic, "row.names")[1] == "null")
    {sp_R2_1 <- rsq(null, adj = T)}
else 
    {sp_R2_1 <- rsq(mult, adj = T)}

# row 2 R2
if(attr(sp_aic, "row.names")[2] == "sev")
    {sp_R2_2 <- rsq(sev, adj = T)}
if(attr(sp_aic, "row.names")[2] == "tsf")
    {sp_R2_2 <- rsq(tsf, adj = T)}
if(attr(sp_aic, "row.names")[2] == "add")
    {sp_R2_2 <- rsq(add, adj = T)}
if(attr(sp_aic, "row.names")[2] == "null")
    {sp_R2_5 <- rsq(null, adj = T)}
else 
    {sp_R2_2 <- rsq(mult, adj = T)}

# row 3 R2
if(attr(sp_aic, "row.names")[3] == "sev")
    {sp_R2_3 <- rsq(sev, adj = T)}
if(attr(sp_aic, "row.names")[3] == "tsf")
    {sp_R2_3 <- rsq(tsf, adj = T)}
if(attr(sp_aic, "row.names")[3] == "add")
    {sp_R2_3 <- rsq(add, adj = T)}
if(attr(sp_aic, "row.names")[3] == "null")
    {sp_R2_5 <- rsq(null, adj = T)}
else 
    {sp_R2_3 <- rsq(mult, adj = T)}

# row 4 R2
if(attr(sp_aic, "row.names")[4] == "sev")
    {sp_R2_4 <- rsq(sev, adj = T)}
if(attr(sp_aic, "row.names")[4] == "tsf")
    {sp_R2_4 <- rsq(tsf, adj = T)}
if(attr(sp_aic, "row.names")[4] == "add")
    {sp_R2_4 <- rsq(add, adj = T)}
if(attr(sp_aic, "row.names")[4] == "null")
    {sp_R2_5 <- rsq(null, adj = T)}
else 
    {sp_R2_4 <- rsq(mult, adj = T)}

# row 5 R2
if(attr(sp_aic, "row.names")[5] == "sev")
    {sp_R2_5 <- rsq(sev, adj = T)}
if(attr(sp_aic, "row.names")[5] == "tsf")
    {sp_R2_5 <- rsq(tsf, adj = T)}
if(attr(sp_aic, "row.names")[5] == "add")
    {sp_R2_5 <- rsq(add, adj = T)}
if(attr(sp_aic, "row.names")[5] == "null")
    {sp_R2_5 <- rsq(null, adj = T)}
else 
    {sp_R2_5 <- rsq(mult, adj = T)}

# Likelihood Ratio Test
# row 1 R2
if(attr(sp_aic, "row.names")[1] == "sev")
    {sp_lrt_1 <- lrtest(sev, sp_null)}
if(attr(sp_aic, "row.names")[1] == "tsf")
    {sp_lrt_1 <- lrtest(tsf, sp_null)}
if(attr(sp_aic, "row.names")[1] == "add")
    {sp_lrt_1 <- lrtest(add, sp_null)}
if(attr(sp_aic, "row.names")[1] == "null")
    {sp_lrt_1 <- lrtest(null, sp_null)}
else 
    {sp_lrt_1 <- lrtest(mult, sp_null)}

# row 2 R2
if(attr(sp_aic, "row.names")[2] == "sev")
    {sp_lrt_2 <- lrtest(sev, sp_null)}
if(attr(sp_aic, "row.names")[2] == "tsf")
    {sp_lrt_2 <- lrtest(tsf, sp_null)}
if(attr(sp_aic, "row.names")[2] == "add")
    {sp_lrt_2 <- lrtest(add, sp_null)}
if(attr(sp_aic, "row.names")[2] == "null")
    {sp_lrt_2 <- lrtest(null, sp_null)}
else 
    {sp_lrt_2 <- lrtest(mult, sp_null)}

# row 3 R2
if(attr(sp_aic, "row.names")[3] == "sev")
    {sp_lrt_3 <- lrtest(sev, sp_null)}
if(attr(sp_aic, "row.names")[3] == "tsf")
    {sp_lrt_3 <- lrtest(tsf, sp_null)}
if(attr(sp_aic, "row.names")[3] == "add")
    {sp_lrt_3 <- lrtest(add, sp_null)}
if(attr(sp_aic, "row.names")[3] == "null")
    {sp_lrt_3 <- lrtest(null, sp_null)}
else 
    {sp_lrt_3 <- lrtest(mult, sp_null)}

# row 4 R2
if(attr(sp_aic, "row.names")[4] == "sev")
    {sp_lrt_4 <- lrtest(sev, sp_null)}
if(attr(sp_aic, "row.names")[4] == "tsf")
    {sp_lrt_4 <- lrtest(tsf, sp_null)}
if(attr(sp_aic, "row.names")[4] == "add")
    {sp_lrt_4 <- lrtest(add, sp_null)}
if(attr(sp_aic, "row.names")[4] == "null")
    {sp_lrt_4 <- lrtest(null, sp_null)}
else 
    {sp_lrt_4 <- lrtest(mult, sp_null)}

# row 5 R2
if(attr(sp_aic, "row.names")[5] == "sev")
    {sp_lrt_5 <- lrtest(sev, sp_null)}
if(attr(sp_aic, "row.names")[5] == "tsf")
    {sp_lrt_ <- lrtest(tsf, sp_null)}
if(attr(sp_aic, "row.names")[5] == "add")
    {sp_lrt_5 <- lrtest(add, sp_null)}
if(attr(sp_aic, "row.names")[5] == "null")
    {sp_lrt_5 <- lrtest(null, sp_null)}
else 
    {sp_lrt_5 <- lrtest(mult, sp_null)}


#sp_lrt_full <- lrtest(full, sp_null)
#sp_lrt_sev <- lrtest(sev, sp_null)
#sp_lrt_tsfcat <- lrtest(tsf, sp_null)

#moran's I (first load function)
sp_mi <- (moran.test(sp_dat[,2], lwW, alternative="two.sided"))$statistic
sp_mip <- (moran.test(sp_dat[,2], lwW, alternative="two.sided"))$'p.value'

# construct table
sp_tbl1 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[1], sp_aic$dAICc[1], sp_aic$weight[1], sp_lrt_1$Chisq[2], sp_lrt_1$'#Df'[1], sp_lrt_1$'Pr(>Chisq)'[2], sp_R2_1, sp_mi, sp_mip)

sp_tbl2 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[2], sp_aic$dAICc[2], sp_aic$weight[2], sp_lrt_2$Chisq[2], sp_lrt_2$'#Df'[1], sp_lrt_2$'Pr(>Chisq)'[2], sp_R2_2, sp_mi, sp_mip)

sp_tbl3 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[3], sp_aic$dAICc[3], sp_aic$weight[3], sp_lrt_3$Chisq[2], sp_lrt_3$'#Df'[1], sp_lrt_3$'Pr(>Chisq)'[2], sp_R2_3, sp_mi, sp_mip)

sp_tbl4 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[4], sp_aic$dAICc[4], sp_aic$weight[4], sp_lrt_4$Chisq[2], sp_lrt_4$'#Df'[1], sp_lrt_4$'Pr(>Chisq)'[2], sp_R2_4, sp_mi, sp_mip)

sp_tbl5 <- c(colnames(sp_dat)[2], attr(sp_aic, "row.names")[5], sp_aic$dAICc[5], sp_aic$weight[5], sp_lrt_5$Chisq[2], sp_lrt_5$'#Df'[1], sp_lrt_5$'Pr(>Chisq)'[2], sp_R2_5, sp_mi, sp_mip)

tbl <- data.frame(rbind(tbl_headr, sp_tbl1, sp_tbl2, sp_tbl3, sp_tbl4, sp_tbl5)) 

# Turn first row into column names
names(tbl) <- lapply(tbl[1, ], as.character)
tbl <- tbl[-1,]

# convert to numeric
tbl[,c(3:10)] <- as.numeric(as.character(unlist(tbl[,c(3:10)]))) 

#remove row names
rownames(tbl) <- c()

tbl
}
```

# SPP with more than 10 detections
I also removed all that were not significant in the NMDS
```{r}
plant_dat_sum <- rbind(plant_dat_pa[,-1], colSums(plant_dat_pa[-1,-1]))

plant_sum <- data.frame(t(plant_dat_sum[-c(1:48),])) 

plant_sum <- cbind(rownames(plant_sum), data.frame(plant_sum, row.names=NULL))

colnames(plant_sum) <- c("species", "sum")

plant_sum <- plant_sum %>% 
  arrange(desc(sum))

# NMDS significant species --- But is is a good idea anyway? I don't even know what those numbers mean!
ar <- data.frame(A$arrows)
r <- data.frame(A$r)
pv <- data.frame(A$pvals)
NMDS_signif_plant2 <- cbind(ar, r, pv)

NMDS_signif_plant1 <- cbind(rownames(NMDS_signif_plant2), data.frame(NMDS_signif_plant2, row.names=NULL))

colnames(NMDS_signif_plant1) <- c("species", "NMDS1", "NMDS2", "r", "pval")

NMDS_signif_plant <- NMDS_signif_plant1 %>% 
  left_join(plant_sum, by = "species") %>% 
  filter(sum >= 5)# %>% 
  #filter(pval <= 0.05)

print(NMDS_signif_plant[,1])

write.csv(NMDS_signif_plant, "more_plant_models.csv")

```


# ACMA (NS)
```{r ACMA}
plant_glm_cov <- plant_glm_pa # instead of changing each chunk, use this if using p/a instead of cover

sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ACMA, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(ACMA/100 ~ sev, sp_dat, family = gaussian)
sp_mod2 <- glm(ACMA/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot1(sp_mod1)
occ_plot2(sp_mod2)
```

# ANMAD
```{r ANMAD}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ANMAD, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(ANMAD ~ sev, sp_dat, family = binomial)
#sp_mod2 <- glm(ANMAD/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
#occ_plot2(sp_mod2)
```

# APAN
```{r APAN}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, APAN, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(APAN/100 ~ sev, sp_dat, family = gaussian)
sp_mod2 <- glm(APAN/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
occ_plot2(sp_mod2)
```

# ARDI
```{r ARDI}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ARDI, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)
```

# ARME
```{r ARME}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ARME, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(ARME/100 ~ sev + tsf_cat, sp_dat, family = gaussian)
sp_mod2 <- glm(ARME/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot1(sp_mod1)
occ_plot2(sp_mod2)
```
# CASC
```{r CASC}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, CASC, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(CASC ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# CEIN
```{r CEIN}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, CEIN, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(CEIN ~ sev, sp_dat, family = binomial(link = "logit"))
#sp_mod2 <- glm(CEIN/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
#occ_plot2(sp_mod2)
```
# COCO
```{r COCO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, COCO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# COHE
```{r COHE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, COHE, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# CONU
```{r CONU}
# WHy are the estimates from the model so small? they don't add up to even 10%!
sp_dat <- plant_glm_pa %>% 
  dplyr::select(site_id, CONU, tsf_cat2, sev, tsf, tsf_cat2, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(CONU ~ sev, sp_dat, family = binomial)
sp_mod2 <- glm(CONU/100 ~ tsf_cat, sp_dat, family = gaussian)
sp_mod3 <- glm(CONU/100 ~ sev + tsf_cat, sp_dat, family = gaussian)
sp_mod4 <- glm(CONU/100 ~ sev + tsf_cat2, sp_dat, family = gaussian)
sp_mod5 <- glm(CONU/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
occ_plot_tsf(sp_mod2)
occ_plot_full(sp_mod3)
occ_plot_full2cat(sp_mod4)
occ_plot_sevtsf(sp_mod5)

summary(sp_mod3)

logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

```
# ELGL
```{r ELGL}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ELGL, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# FECA
```{r FECA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, FECA, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# FEOC
```{r FEOC}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, FEOC, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# GABO
```{r GABO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, GABO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# GATR
```{r GATR}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, GATR, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(GATR ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# HIAL
```{r HIAL}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, HIAL, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# IRTE
```{r IRTE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, IRTE, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# LOHI
```{r LOHI}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, LOHI, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# MAGR
```{r MAGR}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, MAGR, tsf_cat2, sev)

lmt(sp_dat)
```

# MANE
```{r MAGR}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, MANE, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# MECA
```{r MECA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, MECA, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(MECA ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# NODE (S but lame)
```{r NODE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, NODE, tsf_cat2, sev, tsf, tsf_cat2)

lmt(sp_dat)

sp_mod1 <- glm(NODE/100 ~ sev, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
```

# PILA
```{r PILA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PILA, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# PIPO
```{r PIPO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PIPO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# POCO
```{r POCO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, POCO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# POIM
```{r POIM}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, POIM, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# PRHO
```{r PRHO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PRHO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# PSME (S)
```{r PSME}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PSME, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(PSME/100 ~ sev, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)

sp_mod1 <- glm(PSME/100 ~ tsf_cat, sp_dat, family = gaussian)
occ_plot_tsf(sp_mod1)
```
# PTAQ
```{r PTAQ}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PTAQ, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# QUCH
```{r QUCH}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, QUCH, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# QUKE
```{r QUKE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, QUKE, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(QUKE/100 ~ tsf_cat, sp_dat, family = gaussian)
occ_plot_tsf(sp_mod1)
```
# RIRO
```{r RIRO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, RIRO, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(RIRO ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# ROGY
```{r ROGY}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ROGY, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(ROGY ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# RULE
```{r RULE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, RULE, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(RULE ~ tsf_cat2, sp_dat, family = binomial(link = "logit"))
occ_plot_tsf_cat2(sp_mod1)
```
# RUPA
```{r RUPA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, RUPA, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# RUUR
```{r RUUR}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, RUUR, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# SYMO
```{r SYMO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, SYMO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# TODI
```{r TODI}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, TODI, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# TRLA
```{r TRLA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, TRLA, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# WHMO
```{r WHMO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, WHMO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

```{r}

#Goodness-of-fit tests
#This runs three GOF tests: Chi square, Freeman Tukey, and SSE. What you're basically looking for is that your data doesn't fall near the extremes of the bootstrapped distribution. A good threshold for each statistic might be >.1 and <0.9. Probably good to run at least 100 simulations, more is preferable (nsim in the bottom code). These can be slow.

#m12 is your model fit - if this doesn't work, you made need to play around with the model output you are using and figure out how to extract the required elements below
#fm <- m12

#fm <- sp_mod

#Function for the GOF tests - if this doesn't work, check your model output and make sure observed, expected, and residuals are being properly extracted and amend those first three lines as needed. I haven't tried this with a GLM output, but it should be an easy fix if it doesn't work out of the box.
#looks like boot() needs some sort of indices parameter... not sure what this means but this seems to work:
fitstats <- function(sp_dat, ind) {
  fm <- glm(sp_dat[,2]/100 ~ sev + tsf_cat, sp_dat[ind,], family = "poisson")
  observed <- fm$y
  expected <- fitted(fm)
  resids <- residuals(fm)
  sse <- sum(resids^2)
  chisq <- sum((observed - expected)^2 / expected)
  freeTuke <- sum((sqrt(observed) - sqrt(expected))^2)
  out <- c(SSE=sse, Chisq=chisq, freemanTukey=freeTuke)
  return(out)
}


fitstats(sp_dat)

#Bootstrapping the GOF - this is using parboot() from Unmarked. It may not play nicely with GLM output. If not, try the boot() function from the boot package in place of parboot(). I think the syntax is nearly identical except maybe the 'report' part which isn't important.
library(boot)
pb <- boot(sp_dat, fitstats, R = 100)
#how to interpret?

#Plot isn't necessary but might help you understand the outputs - may need some tweeking to work for boot()
plot(pb)




#Next section

#Converting logit scale to natural scale. The link function for binomial is logit, so you need to backtransform to get from that scale to a probability of occurence (0-1). Use the following function. Be sure to add together any coefficients and intercepts first - all arithmetic should be done before backtransformation:

logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}



#If you do anything that uses Poisson, that's on the log scale so you can just use exp() to convert to the natural scale.
```


