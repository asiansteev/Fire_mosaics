---
title: "Test"
author: "Chris Adlam"
date: "7/20/2018"
output: html_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Packages
```{r setup, include=FALSE}
if(!require('pacman'))install.packages('pacman')
pacman::p_load(tidyverse, emmeans, ggplot2, cowplot, vegan, data.table, kableExtra, plyr, nloptr, labdsv, bbmle, spdep, rsq,lmtest, unmarked, broom)

#library(dplyr)
#library(ggplot2)
library(purrr)
library(tibble)
#library(tidyr)
library(lmtest)
library(bbmle)
library(spdep)
library(rsq)
library(lmtest)
#options(contrasts =c("contr.sum", "contr.poly"))
#options(contrasts = rep ("contr.treatment", 2))

# load scripts
source("scripts/occ_plot.R")
source("scripts/lmt.R")
```

# Bird data
```{r}
# read in bird data
bird_dat_count <- read.csv("data/bird_data.csv", header = T) %>% 
  filter(DetectionLocationNm != "O") %>% # removing species outside (O) the stand
  dplyr::select(Point, Count, Spp, DistanceBin) %>%  # keeping only relevant columns
  dplyr::rename(distance = DistanceBin)

# remove duplicate rows (same species detected multiple times in a single plot)
bird_dat_long <- bird_dat_count %>% 
  dplyr::rename(site_id = Point) %>% 
  dplyr::rename(species = Spp)
```

# Plant and site data
```{r include=F}
plant_data <- read.csv("/Users/christopheradlam/Desktop/Davis/R/GitHub Repos/Fire_mosaics/Data/plant_data.csv")
plant_names <- read.csv("/Users/christopheradlam/Desktop/Davis/R/GitHub Repos/Fire_mosaics/data/plant_list.csv")
plant_data$species <- as.character(plant_data$species)
plant_data$site_id <- as.character(plant_data$site_id)
plant_names$species <- as.character(plant_names$species)
plant_names$full_name <- as.character(plant_names$full_name)
plant_names$native_status <- as.factor(plant_names$native_status)
plant_names$form <- as.factor(plant_names$form)

# Keep only native species/ long format
plant_dat <- left_join(plant_data, plant_names, by = "species") %>%
  filter(native_status == "native") %>%
  dplyr::select(site_id, species, cover)

site_data <- read.csv("data/site_data.csv")

# Set tsf_cat in site_data NOT WORKING
site_data$tsf <- as.numeric(as.character(site_data$tsf))

site_data <- site_data %>% 
    mutate(tsf_cat2 = ifelse(is.na(tsf), "3", "2"))

site_data$tsf_cat2[site_data$tsf < 15] <-1


# Data prep
# convert to wide format for following analysis
plant_matrix1 <- spread(data = plant_dat, key = species, value = cover, fill = 0)

# Make Bray-Curtis dissimilarity matrix
plants_matrix <- as.matrix(plant_matrix1[, -1])
```

```{r get plant data}
#install.packages("fuzzySim") doesnt work, so here's the function; serves to convert from presence only to presence-absence

splist2presabs <- function(data, sites.col, sp.col, keep.n = FALSE) {
  # version 1.1 (7 May 2013)
  # data: a matrix or data frame with your localities and species (each in a different column)
  # sites.col: the name or index number of the column containing the localities
  # sp.col: the name or index number of the column containing the species names or codes
  # keep.n: logical, whether to get in the resulting table the number of times each species appears in each locality; if false (the default), only the presence (1) or absence (0) are recorded

  stopifnot(
    length(sites.col) == 1,
    length(sp.col) == 1,
    sites.col != sp.col,
    sites.col %in% 1 : ncol(data) | sites.col %in% names(data),
    sp.col %in% 1 : ncol(data) | sp.col %in% names(data),
    is.logical(keep.n)
  )

  presabs <- table(data[ , c(sites.col, sp.col)])
  presabs <- as.data.frame(unclass(presabs))
  if (!keep.n)  presabs[presabs > 1] <- 1
  presabs <- data.frame(row.names(presabs), presabs)
  names(presabs)[1] <- names(subset(data, select = sites.col))
  rownames(presabs) <- NULL
  return(presabs)
}  # end splist2presabs function

# executing function and going from wide to long:
plant_dat_pa <- splist2presabs(plant_dat, sites.col = 1, sp.col = 2) 

# data for plant glm using p/a
plant_glm_pa <- merge(plant_dat_pa, site_data, by= 'site_id') %>% 
  mutate(tsf_cat = as.factor(as.character(tsf_cat)))  %>% # if tsf_cat is numeric, model output is gibberish; must change to factor
  mutate(sev_tsf = paste(sev, tsf_cat2, sep = "-"))

plant_glm_pa$tsf_cat <- as.factor(plant_glm_pa$tsf_cat)

# For cover instead of p/a, use the following, which is the same as plant_mrpp_d:
site_data$site_id <- as.character(site_data$site_id) 
plant_glm_cov <- left_join(plant_matrix1, site_data, by = "site_id") %>% 
  mutate(sev_tsf = paste(sev, tsf_cat2, sep = "-"))

plant_glm_cov$tsf_cat <- as.factor(plant_glm_cov$tsf_cat)
```

# Cluster analysis 1
```{r}
# Cluster Analysis
mydata <- 
str(mydata)
head(mydata)
pairs(mydata)

# Scatter plot 
plot(mydata$Fuel_Cost~ mydata$Sales, data = mydata)
with(mydata,text(mydata$Fuel_Cost ~ mydata$Sales, labels=mydata$Company,pos=4))

# Normalize 
z = mydata[,-c(1,1)]
means = apply(z,2,mean)
sds = apply(z,2,sd)
nor = scale(z,center=means,scale=sds)

##calculate distance matrix (default is Euclidean distance)
distance = dist(nor)

# Hierarchical agglomerative clustering using default complete linkage 
mydata.hclust = hclust(distance)
plot(mydata.hclust)
plot(mydata.hclust,labels=mydata$Company,main='Default from hclust')
plot(mydata.hclust,hang=-1)

# Hierarchical agglomerative clustering using "average" linkage 
mydata.hclust<-hclust(distance,method="average")
plot(mydata.hclust,hang=-1)

# Cluster membership
member = cutree(mydata.hclust,3)
table(member)

#Characterizing clusters 
aggregate(nor,list(member),mean)
aggregate(mydata[,-c(1,1)],list(member),mean)

# Scree Plot
wss <- (nrow(nor)-1)*sum(apply(nor,2,var))
for (i in 2:20) wss[i] <- sum(kmeans(nor, centers=i)$withinss)
plot(1:20, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares") 

# K-means clustering
kc<-kmeans(nor,3)
```


# Cluster analysis 2
```{r}
# the cluster analysis is to figure out the TSF thresholds; also at what point are UN and LS site indistinguishable? how about UN/LS and HS?


#install.packages(c("cluster", "factoextra"))
library(cluster)
library(factoextra)

#df <- plant_glm_cov[, c(1:139)]

#df <- subset(df, select = -c(CESA, ALRH, SALU))

df <- all_spp_w_pa[, c(1:237)]

df1 <- df[,-1]
rownames(df1) <- df[,1]

df.scaled <- scale(df1)

#trying spearman correlation but returns all NAs
dist_corr <- get_dist(df.scaled, stand = TRUE, method = "spearman")

#Euclidean dist instead
dist.eucl <- dist(df.scaled, method = "euclidean")
#fviz_dist(dist.eucl) # visualization

# for some reason this wouldn't work until I ran the fire_mosaics.Rmd....
# But when scaled it just suggests 1 cluster vs 6 when not clustered
mydata <- as.matrix(df1[, -c(5,223)])
fviz_nbclust(mydata, kmeans, method = "gap_stat")

km.res <- kmeans(mydata, 4) #, nstart = 25)
#which(apply(mydata, 2, var)==0) # to find columns with 0 variance

# Visualize
fviz_cluster(km.res, data = mydata, palette = "jco",
             ggtheme = theme_minimal())

clustbl <- as.data.frame(km.res$cluster)

```
# Cluster analysis 2 TSF
```{r}
# the cluster analysis is to figure out the TSF thresholds; also at what point are UN and LS site indistinguishable? how about UN/LS and HS?


#install.packages(c("cluster", "factoextra"))
library(cluster)
library(factoextra)

plant_glm_cov1 <- plant_glm_cov %>% 
  filter(sev== "l")

df <- plant_glm_cov1[, c(1:139)] 

df1 <- df[,-1]
rownames(df1) <- df[,1]

nosp <- as.data.frame(which(apply(df1, 2, var) == 0)) # to find columns with 0 variance
nosp <- cbind(rownames(nosp), data.frame(nosp, row.names=NULL)) # change row names to column

# remove spp not in lS
df2 <- subset(df1, select = -c(nosp[,1]))

#df1 <- df[,-1]
#rownames(df1) <- df[,1]

df.scaled <- scale(df2)

#trying spearman correlation but returns all NAs
dist_corr <- get_dist(df.scaled, stand = TRUE, method = "spearman")

#Euclidean dist instead
dist.eucl <- dist(df.scaled, method = "euclidean")
#fviz_dist(dist.eucl) # visualization

# for some reason this wouldn't work until I ran the fire_mosaics.Rmd....
# But when scaled it just suggests 1 cluster vs 6 when not clustered
mydata <- as.matrix(df.scaled)
fviz_nbclust(mydata, kmeans, method = "gap_stat")

km.res <- kmeans(mydata, 3) #, nstart = 25)

# Visualize
fviz_cluster(km.res, data = mydata, palette = "jco",
             ggtheme = theme_minimal())

```


```{r Moran's I}
#library(ape)

dists <- as.matrix(dist(cbind(plant_glm_cov$lon, plant_glm_cov$lat)))
#dists.inv <- 1/dists
#diag(dists.inv) <- 0
#Moran.I(plant_mrpp_d$PSME, dists.inv, na.rm = T)

# convert w to a row standardised general weights object
lw <- mat2listw(dists)
lwW <- nb2listw(lw$neighbours, glist=lw$weights, style="W")
CEIN_mi <- (moran.test(plant_glm_cov$MECA, lwW, alternative="two.sided"))$statistic

#CEIN_mi <- (moran.test(MECA_dat[,2], lwW, alternative="two.sided"))$statistic

CEIN_mip <- (moran.test(plant_glm_cov$MECA, lwW, alternative="two.sided"))$'p.value'
```

```{r AIC test}
d <- as.data.frame(UCBAdmissions)
d <- tidyr::spread(d, Admit, Freq) # use Hadley's excellent tidyr to reshape
d[order(d$Dept), ]

m1 <- glm(cbind(Admitted, Rejected) ~ Gender, d, family='binomial')
m2 <- glm(cbind(Admitted, Rejected) ~ Dept, d, family = 'binomial')
m3 <- glm(cbind(Admitted, Rejected) ~ Dept + Gender, d, family = 'binomial')
model.names <- c("1 Gender", "2 Dept", "3 Gender + Dept")

summ.table <- do.call(rbind, lapply(list(m1, m2, m3), broom::glance))

table.cols <- c("df.residual", "deviance", "AIC")
reported.table <- summ.table[table.cols]
names(reported.table) <- c("Resid. Df", "Resid. Dev", "AIC")

reported.table[['dAIC']] <-  with(reported.table, AIC - min(AIC))
reported.table[['weight']] <- with(reported.table, exp(- 0.5 * dAIC) / sum(exp(- 0.5 * dAIC)))
reported.table$AIC <- NULL
reported.table$weight <- round(reported.table$weight, 2)
reported.table$dAIC <- round(reported.table$dAIC, 1)
row.names(reported.table) <- model.names

```


# SPP with more than 10 detections
I also removed all that were not significant in the NMDS
```{r}
plant_dat_sum <- rbind(plant_dat_pa[,-1], colSums(plant_dat_pa[-1,-1]))

plant_sum <- data.frame(t(plant_dat_sum[-c(1:48),])) 

plant_sum <- cbind(rownames(plant_sum), data.frame(plant_sum, row.names=NULL))

colnames(plant_sum) <- c("species", "sum")

plant_sum <- plant_sum %>% 
  arrange(desc(sum))

# NMDS significant species --- But is is a good idea anyway? I don't even know what those numbers mean!
ar <- data.frame(A$arrows)
r <- data.frame(A$r)
pv <- data.frame(A$pvals)
NMDS_signif_plant2 <- cbind(ar, r, pv)

NMDS_signif_plant1 <- cbind(rownames(NMDS_signif_plant2), data.frame(NMDS_signif_plant2, row.names=NULL))

colnames(NMDS_signif_plant1) <- c("species", "NMDS1", "NMDS2", "r", "pval")

NMDS_signif_plant <- NMDS_signif_plant1 %>% 
  left_join(plant_sum, by = "species") %>% 
  filter(sum >= 5)# %>% 
  #filter(pval <= 0.05)

print(NMDS_signif_plant[,1])

write.csv(NMDS_signif_plant, "more_plant_models.csv")

```


# ACMA (NS)
```{r ACMA}
plant_glm_cov <- plant_glm_pa # instead of changing each chunk, use this if using p/a instead of cover

sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ACMA, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(ACMA/100 ~ sev, sp_dat, family = gaussian)
sp_mod2 <- glm(ACMA/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot1(sp_mod1)
occ_plot2(sp_mod2)
```

# ANMAD
```{r ANMAD}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ANMAD, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(ANMAD ~ sev, sp_dat, family = binomial)
#sp_mod2 <- glm(ANMAD/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
#occ_plot2(sp_mod2)
```

# APAN
```{r APAN}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, APAN, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(APAN/100 ~ sev, sp_dat, family = gaussian)
sp_mod2 <- glm(APAN/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
occ_plot2(sp_mod2)
```

# ARDI
```{r ARDI}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ARDI, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)
```

# ARME
```{r ARME}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ARME, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(ARME/100 ~ sev + tsf_cat, sp_dat, family = gaussian)
sp_mod2 <- glm(ARME/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot1(sp_mod1)
occ_plot2(sp_mod2)
```
# CASC
```{r CASC}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, CASC, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(CASC ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# CEIN
```{r CEIN}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, CEIN, tsf_cat2, sev, tsf, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(CEIN ~ sev, sp_dat, family = binomial(link = "logit"))
#sp_mod2 <- glm(CEIN/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
#occ_plot2(sp_mod2)
```
# COCO
```{r COCO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, COCO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# COHE
```{r COHE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, COHE, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# CONU
```{r CONU}
# WHy are the estimates from the model so small? they don't add up to even 10%!
sp_dat <- plant_glm_pa %>% 
  dplyr::select(site_id, CONU, tsf_cat2, sev, tsf, tsf_cat2, sev_tsf)

lmt(sp_dat)

sp_mod1 <- glm(CONU ~ sev, sp_dat, family = binomial)
sp_mod2 <- glm(CONU/100 ~ tsf_cat, sp_dat, family = gaussian)
sp_mod3 <- glm(CONU/100 ~ sev + tsf_cat, sp_dat, family = gaussian)
sp_mod4 <- glm(CONU/100 ~ sev + tsf_cat2, sp_dat, family = gaussian)
sp_mod5 <- glm(CONU/100 ~ sev_tsf, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
occ_plot_tsf(sp_mod2)
occ_plot_full(sp_mod3)
occ_plot_full2cat(sp_mod4)
occ_plot_sevtsf(sp_mod5)

summary(sp_mod3)

logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}

```
# ELGL
```{r ELGL}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ELGL, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# FECA
```{r FECA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, FECA, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# FEOC
```{r FEOC}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, FEOC, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# FEMI
```{r FEOC}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, FEMI, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# GABO
```{r GABO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, GABO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# GATR
```{r GATR}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, GATR, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(GATR ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# HIAL
```{r HIAL}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, HIAL, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# IRTE
```{r IRTE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, IRTE, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# LOHI
```{r LOHI}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, LOHI, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# MAGR
```{r MAGR}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, MAGR, tsf_cat2, sev)

lmt(sp_dat)
```

# MANE
```{r MAGR}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, MANE, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# MECA
```{r MECA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, MECA, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(MECA ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# NODE (S but lame)
```{r NODE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, NODE, tsf_cat2, sev, tsf, tsf_cat2)

lmt(sp_dat)

sp_mod1 <- glm(NODE/100 ~ sev, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)
```

# PILA
```{r PILA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PILA, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# PIPO
```{r PIPO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PIPO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# POCO
```{r POCO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, POCO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# POIM
```{r POIM}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, POIM, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# PRHO
```{r PRHO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PRHO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# PSME (S)
```{r PSME}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PSME, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(PSME/100 ~ sev, sp_dat, family = gaussian)
occ_plot_sev(sp_mod1)

sp_mod1 <- glm(PSME/100 ~ tsf_cat, sp_dat, family = gaussian)
occ_plot_tsf(sp_mod1)
```
# PTAQ
```{r PTAQ}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, PTAQ, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# QUCH
```{r QUCH}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, QUCH, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# QUKE
```{r QUKE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, QUKE, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(QUKE/100 ~ tsf_cat, sp_dat, family = gaussian)
occ_plot_tsf(sp_mod1)
```
# RIRO
```{r RIRO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, RIRO, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(RIRO ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# ROGY
```{r ROGY}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, ROGY, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(ROGY ~ sev, sp_dat, family = binomial(link = "logit"))
occ_plot_sev(sp_mod1)
```
# RULE
```{r RULE}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, RULE, tsf_cat2, sev, tsf)

lmt(sp_dat)

sp_mod1 <- glm(RULE ~ tsf_cat2, sp_dat, family = binomial(link = "logit"))
occ_plot_tsf_cat2(sp_mod1)
```
# RUPA
```{r RUPA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, RUPA, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# RUUR
```{r RUUR}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, RUUR, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# SYMO
```{r SYMO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, SYMO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```
# TODI
```{r TODI}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, TODI, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# TRLA
```{r TRLA}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, TRLA, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

# WHMO
```{r WHMO}
sp_dat <- plant_glm_cov %>% 
  dplyr::select(site_id, WHMO, tsf_cat2, sev, tsf)

lmt(sp_dat)
```

```{r}

#Goodness-of-fit tests
#This runs three GOF tests: Chi square, Freeman Tukey, and SSE. What you're basically looking for is that your data doesn't fall near the extremes of the bootstrapped distribution. A good threshold for each statistic might be >.1 and <0.9. Probably good to run at least 100 simulations, more is preferable (nsim in the bottom code). These can be slow.

#m12 is your model fit - if this doesn't work, you made need to play around with the model output you are using and figure out how to extract the required elements below
#fm <- m12

#fm <- sp_mod

#Function for the GOF tests - if this doesn't work, check your model output and make sure observed, expected, and residuals are being properly extracted and amend those first three lines as needed. I haven't tried this with a GLM output, but it should be an easy fix if it doesn't work out of the box.
#looks like boot() needs some sort of indices parameter... not sure what this means but this seems to work:
fitstats <- function(sp_dat, ind) {
  fm <- glm(sp_dat[,2]/100 ~ sev + tsf_cat, sp_dat[ind,], family = "poisson")
  observed <- fm$y
  expected <- fitted(fm)
  resids <- residuals(fm)
  sse <- sum(resids^2)
  chisq <- sum((observed - expected)^2 / expected)
  freeTuke <- sum((sqrt(observed) - sqrt(expected))^2)
  out <- c(SSE=sse, Chisq=chisq, freemanTukey=freeTuke)
  return(out)
}


fitstats(sp_dat)

#Bootstrapping the GOF - this is using parboot() from Unmarked. It may not play nicely with GLM output. If not, try the boot() function from the boot package in place of parboot(). I think the syntax is nearly identical except maybe the 'report' part which isn't important.
library(boot)
pb <- boot(sp_dat, fitstats, R = 100)
#how to interpret?

#Plot isn't necessary but might help you understand the outputs - may need some tweeking to work for boot()
plot(pb)




#Next section

#Converting logit scale to natural scale. The link function for binomial is logit, so you need to backtransform to get from that scale to a probability of occurence (0-1). Use the following function. Be sure to add together any coefficients and intercepts first - all arithmetic should be done before backtransformation:

logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds / (1 + odds)
  return(prob)
}



#If you do anything that uses Poisson, that's on the log scale so you can just use exp() to convert to the natural scale.
```


